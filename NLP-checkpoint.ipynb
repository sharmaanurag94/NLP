{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u> NLP</u></h2>\n",
    "\n",
    "#  Natural Language Processing\n",
    "\n",
    "In this demo, you will be shown how to perform various techniques learnt throughout this module using libraries from Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Types Of Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/edureka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/edureka/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/edureka/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'Let', \"'s\", 'use', 'NLTK', '.']\n",
      "['Hello', '!', 'Let', \"'\", 's', 'use', 'NLTK', '.']\n",
      "['Hello!', \"Let's use NLTK.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/edureka/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "s = \"Hello! Let's use NLTK.\"\n",
    "print(word_tokenize(s))\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "print(wordpunct_tokenize(s))\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "print(sent_tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Tokens Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "#let us consider the below string for the example\n",
    "string = \"ML can be seen as a time-saving device that allows humans to explore their more creative ambitions while ML is in the background crunching numbers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ML',\n",
       " 'can',\n",
       " 'be',\n",
       " 'seen',\n",
       " 'as',\n",
       " 'a',\n",
       " 'time-saving',\n",
       " 'device',\n",
       " 'that',\n",
       " 'allows',\n",
       " 'humans',\n",
       " 'to',\n",
       " 'explore',\n",
       " 'their',\n",
       " 'more',\n",
       " 'creative',\n",
       " 'ambitions',\n",
       " 'while',\n",
       " 'ML',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " 'background',\n",
       " 'crunching',\n",
       " 'numbers']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_tokens=nltk.word_tokenize(string)\n",
    "ML_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Bigrams And Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ML', 'can'),\n",
       " ('can', 'be'),\n",
       " ('be', 'seen'),\n",
       " ('seen', 'as'),\n",
       " ('as', 'a'),\n",
       " ('a', 'time-saving'),\n",
       " ('time-saving', 'device'),\n",
       " ('device', 'that'),\n",
       " ('that', 'allows'),\n",
       " ('allows', 'humans'),\n",
       " ('humans', 'to'),\n",
       " ('to', 'explore'),\n",
       " ('explore', 'their'),\n",
       " ('their', 'more'),\n",
       " ('more', 'creative'),\n",
       " ('creative', 'ambitions'),\n",
       " ('ambitions', 'while'),\n",
       " ('while', 'ML'),\n",
       " ('ML', 'is'),\n",
       " ('is', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'background'),\n",
       " ('background', 'crunching'),\n",
       " ('crunching', 'numbers')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_bigrams=list(nltk.bigrams(ML_tokens))\n",
    "ML_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ML', 'can', 'be'),\n",
       " ('can', 'be', 'seen'),\n",
       " ('be', 'seen', 'as'),\n",
       " ('seen', 'as', 'a'),\n",
       " ('as', 'a', 'time-saving'),\n",
       " ('a', 'time-saving', 'device'),\n",
       " ('time-saving', 'device', 'that'),\n",
       " ('device', 'that', 'allows'),\n",
       " ('that', 'allows', 'humans'),\n",
       " ('allows', 'humans', 'to'),\n",
       " ('humans', 'to', 'explore'),\n",
       " ('to', 'explore', 'their'),\n",
       " ('explore', 'their', 'more'),\n",
       " ('their', 'more', 'creative'),\n",
       " ('more', 'creative', 'ambitions'),\n",
       " ('creative', 'ambitions', 'while'),\n",
       " ('ambitions', 'while', 'ML'),\n",
       " ('while', 'ML', 'is'),\n",
       " ('ML', 'is', 'in'),\n",
       " ('is', 'in', 'the'),\n",
       " ('in', 'the', 'background'),\n",
       " ('the', 'background', 'crunching'),\n",
       " ('background', 'crunching', 'numbers')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_trigrams=list(nltk.trigrams (ML_tokens))\n",
    "ML_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### POS Tagging - Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ML', 'NN')]\n",
      "[('can', 'MD')]\n",
      "[('be', 'VB')]\n",
      "[('seen', 'VBN')]\n",
      "[('as', 'IN')]\n",
      "[('a', 'DT')]\n",
      "[('time-saving', 'NN')]\n",
      "[('device', 'NN')]\n",
      "[('that', 'IN')]\n",
      "[('allows', 'NNS')]\n",
      "[('humans', 'NNS')]\n",
      "[('to', 'TO')]\n",
      "[('explore', 'NN')]\n",
      "[('their', 'PRP$')]\n",
      "[('more', 'RBR')]\n",
      "[('creative', 'JJ')]\n",
      "[('ambitions', 'NNS')]\n",
      "[('while', 'IN')]\n",
      "[('ML', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('in', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('background', 'NN')]\n",
      "[('crunching', 'VBG')]\n",
      "[('numbers', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "for token in ML_tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shortcomings Of POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jim', 'NNP')]\n",
      "[('eats', 'NNS')]\n",
      "[('a', 'DT')]\n",
      "[('banana', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sent= \"Jim eats a banana\"\n",
    "tokens = word_tokenize(sent)\n",
    "for token in tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jim', 'NNP'),\n",
       " (' ', 'NNP'),\n",
       " ('eats', 'VBZ'),\n",
       " (' ', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " (' ', 'NN'),\n",
       " ('banana', 'NN')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "reg_tokenizer = RegexpTokenizer('(?u)\\W+|\\$[\\d\\.]+|\\S+')\n",
    "regextokens = reg_tokenizer.tokenize(sent)\n",
    "regextags = nltk.pos_tag(regextokens)\n",
    "regextags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words =stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing Stop Words Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ML', 'seen', 'time-saving', 'device', 'allows', 'humans', 'explore', 'creative', 'ambitions', 'ML', 'background', 'crunching', 'numbers']\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence = [w for w in ML_tokens if not w in stop_words] \n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming Using NLTK – Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measur\n",
      "measur\n",
      "measur\n",
      "measur\n",
      "measur\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()\n",
    "print(pst.stem(\"Measure\"))\n",
    "print(pst.stem(\"Measurement\"))\n",
    "print(pst.stem(\"Measuring\"))\n",
    "print(pst.stem(\"Measurer\"))\n",
    "print(pst.stem(\"Measures\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non-English Stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "sbst=SnowballStemmer(\"spanish\")\n",
    "print(sbst.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "produccion\n",
      "product\n"
     ]
    }
   ],
   "source": [
    "print(sbst.stem(\"producción\"))\n",
    "print(sbst.stem(\"producto\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization Using NLTK - Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "eat\n",
      "eat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lem = WordNetLemmatizer()\n",
    "print(word_lem.lemmatize(\"eating\", pos=\"v\"))\n",
    "print(word_lem.lemmatize(\"eats\", pos=\"v\"))\n",
    "print(word_lem.lemmatize(\"ate\", pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of WordNetLemmatizer:  go\n",
      "Result of PorterStemmer:  gone\n",
      "Result of LancasterStemmer:  gon\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "print(\"Result of WordNetLemmatizer: \", word_lem.lemmatize(\"gone\", pos=\"v\"))\n",
    "\n",
    "print(\"Result of PorterStemmer: \", PorterStemmer().stem(\"gone\"))\n",
    "\n",
    "print(\"Result of LancasterStemmer: \", LancasterStemmer().stem(\"gone\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NER Using NLTK - Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/edureka/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/edureka/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/edureka/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/edureka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Ferrari/NNP)\n",
      "  is/VBZ\n",
      "  an/DT\n",
      "  (GPE Italian/JJ)\n",
      "  luxury/NN\n",
      "  sports/NNS\n",
      "  car/NN\n",
      "  manufacturer/NN\n",
      "  based/VBN\n",
      "  in/IN\n",
      "  (GPE Maranello/NNP)\n",
      "  ./.\n",
      "  Founded/VBN\n",
      "  by/IN\n",
      "  (PERSON Enzo/NNP Ferrari/NNP)\n",
      "  in/IN\n",
      "  1939/CD\n",
      "  ,/,\n",
      "  the/DT\n",
      "  company/NN\n",
      "  built/VBD\n",
      "  its/PRP$\n",
      "  first/JJ\n",
      "  car/NN\n",
      "  in/IN\n",
      "  1940/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import ne_chunk\n",
    "\n",
    "ex = \"Ferrari is an Italian luxury sports car manufacturer based in Maranello. Founded by Enzo Ferrari in 1939, the company built its first car in 1940.\"\n",
    "\n",
    "tokenized = nltk.word_tokenize(ex)\n",
    "tagged = nltk.pos_tag(tokenized)\n",
    "namedEnt = ne_chunk(tagged)\n",
    "    \n",
    "print(namedEnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WSD Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('peer.n.01')\n",
      "Synset('match.n.05')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sent1 = \"She is looking for a match\"\n",
    "sent2 = \"Yesterday's Football match was exciting\"\n",
    "\n",
    "print(lesk(word_tokenize(sent1), 'match'))\n",
    "print(lesk(word_tokenize(sent2), 'match'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF Using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_1 = 'The movie was good and we really like it'\n",
    "review_2 = 'the movie was good but the ending was boring'\n",
    "review_3 = 'we did not like the movie as it was too lengthy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_vect = TfidfVectorizer(min_df=1,lowercase=True,stop_words='english')\n",
    "review_1 = 'The movie was good and we really like it'\n",
    "review_2 = 'the movie was good but the ending was boring'\n",
    "review_3 = 'we did not like the movie as it was too lengthy'\n",
    "review_list = [review_1,review_2,review_3]\n",
    "\n",
    "tf_matrix = tf_vect.fit_transform(review_list)\n",
    "tf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>boring</th>\n",
       "      <th>did</th>\n",
       "      <th>ending</th>\n",
       "      <th>good</th>\n",
       "      <th>lengthy</th>\n",
       "      <th>like</th>\n",
       "      <th>movie</th>\n",
       "      <th>really</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.480458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.480458</td>\n",
       "      <td>0.373119</td>\n",
       "      <td>0.631745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.584483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.584483</td>\n",
       "      <td>0.444514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345205</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.584483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.584483</td>\n",
       "      <td>0.444514</td>\n",
       "      <td>0.345205</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     boring       did    ending      good   lengthy      like     movie  \\\n",
       "0  0.000000  0.000000  0.000000  0.480458  0.000000  0.480458  0.373119   \n",
       "1  0.584483  0.000000  0.584483  0.444514  0.000000  0.000000  0.345205   \n",
       "2  0.000000  0.584483  0.000000  0.000000  0.584483  0.444514  0.345205   \n",
       "\n",
       "     really  \n",
       "0  0.631745  \n",
       "1  0.000000  \n",
       "2  0.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tf_names = tf_vect.get_feature_names()\n",
    "tf_df = pd.DataFrame(tf_matrix.toarray(),columns=tf_names)\n",
    "tf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TextBlob For Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.8, subjectivity=0.75)\n",
      "Sentiment(polarity=-0.15, subjectivity=0.4)\n",
      "Sentiment(polarity=1.0, subjectivity=1.0)\n",
      "Sentiment(polarity=-1.0, subjectivity=1.0)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "print(TextBlob('great').sentiment)\n",
    "print(TextBlob('dark').sentiment)\n",
    "print(TextBlob('excellent').sentiment)\n",
    "print(TextBlob('boring').sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Conclusion</i></b>: In this demo, we examined how to apply various text pre-processing techniques using NLTK and TextBlob."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
